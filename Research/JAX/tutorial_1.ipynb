{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9a731e",
   "metadata": {},
   "source": [
    "## **JAX**\n",
    "\n",
    "JAX is a library for array-oriented numerical computation, with automatic differentiation and JIT compilation to enable high-performance machine learning research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c65fe",
   "metadata": {},
   "source": [
    "1. JAX provide a unified NumPy-like interface to computations that run on CPU, GPU or TPI, in local or distributed settings,\n",
    "2. JAX features built-in Jut-in-Time (JIT) compilation, and open source machine learning compiler ecosystem.\n",
    "3. JAX functions support efficient evalution of gradients via its automatic differentiation transformations.\n",
    "4. JAX functions can be automatically vectorized to efficiently map them over arrays representing batches of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7128679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e969f0c",
   "metadata": {},
   "source": [
    "With the above import, we can immediately start using JAX in a similar manner to NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6e1252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.        1.05      2.1       3.1499999 4.2      ]\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(5.0)\n",
    "print(selu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d1e4e",
   "metadata": {},
   "source": [
    "JAX works great for many numerical and scientific programs, but only if they are written with certain constraints, as explained in [tutorial_n.ipynb](#add_link_when_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6837fd",
   "metadata": {},
   "source": [
    "### **Just-in-time compilation with `jax.jit()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d0535",
   "metadata": {},
   "source": [
    "JAX runs transparently on the GPU or TPU (falling back to CPU if you don't have one). However, in the above code, JAX is dispatching kernels to the chip one operation at a time. If we have a sequence of operations, we can use the `jax.jit()` function to compile this sequence of operations together using XLA.\n",
    "\n",
    "\n",
    "We can use python's `%timeit` to quickly benchmark our `selu` function, using `block_until_ready()` to account for JAX's dynamic dispatch. See [tutorial_async](#add_it_too) for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e14fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 μs ± 29.8 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.key(135)\n",
    "x = random.normal(key, (1_000_000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81ffca",
   "metadata": {},
   "source": [
    "We can speed the execution time for this function with `jax.jit()` transformation, which will `jit-compile` the first time `selu` is called and it will be cached forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2420486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221 μs ± 12 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "_ = selu_jit(x)  # warmup\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50deb012",
   "metadata": {},
   "source": [
    "This is just the execution time on CPU, the same code can be run on GPU, or TPU, typically for even greater speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8d0ce9",
   "metadata": {},
   "source": [
    "### Taking derivatives with `jax.grad()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b93091a",
   "metadata": {},
   "source": [
    "In addition to transforming functions via JIT compilation, JAX also provides other transformation. One such transformation is `jax.grad()`, which performs **automatic differentiation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70174f",
   "metadata": {},
   "source": [
    "Automatic Differentiation (AD) is a technique to compute derivatives (graadients) of functions automatically and exactly, using the chain rule. It's not symbolic (like `sympy`), not numerical (like finite differences), it is a programmatic way of doing calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe89f1",
   "metadata": {},
   "source": [
    "Let's say you have a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "433016df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2 + 3*x + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a607a23b",
   "metadata": {},
   "source": [
    "and you want to compute $\\frac{df}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4b911",
   "metadata": {},
   "source": [
    "With AD, you don't have to manually compute that, JAX (or maybe PyTorch) will do it automatically and exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad8ee35",
   "metadata": {},
   "source": [
    "#### Types of AD\n",
    "\n",
    "1. Forward Mode AD:\n",
    "    - Computes derivatives along with the function as it runs\n",
    "    - Efficient for function with few inputs and many outputs\n",
    "2. Reverse Model AD (backpropagation):\n",
    "    - Runs the function forward, and then compute derivatives backward.\n",
    "    - Efficient when you have many inputs and one output (like in Neural nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "478db392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "grad_f = jax.grad(f)\n",
    "print(grad_f(2.0))  # Should print 7.0, which is the derivative of f at x=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c19f333",
   "metadata": {},
   "source": [
    "**JAX** does automatic differentiation using a system of tracing and transformation of Python functions.\n",
    "\n",
    "*JAX doesn't just run your function, it records the operations and applies the chain rule step-by-step using a method called Reverse mode AD*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9041520a",
   "metadata": {},
   "source": [
    "### **Step 1: Tracing**\n",
    "\n",
    "When you call `jax.grad(f)`, JAX doesn't immediately compute the gradient, instead:\n",
    "\n",
    "- It traces the function, meaning runs the function once with a special object called **Tracer** instead of a regular number.\n",
    "- The **Tracer** object records every operation like `x**2`, `+` etc, in a computation graph\n",
    "\n",
    "\n",
    "So our function becomes a chain of elementary operations:\n",
    "\n",
    "$ x \\rightarrow x^2 \\rightarrow 3x \\rightarrow x^2 + 3x \\rightarrow x^2 + 3x + 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11874a91",
   "metadata": {},
   "source": [
    "### **Step 2: Chain Rule (Reverse Model)**\n",
    "\n",
    "\n",
    "JAX walks backward through this computation graph and applies the chain rule to compute how each operation contributed to the final result. For the function\n",
    "\n",
    "$$ f(x) = x^2 + 3x + 2 $$\n",
    "\n",
    "The derivative is:\n",
    "\n",
    "$$ f'(x) = 2x + 3 $$\n",
    "\n",
    "\n",
    "But JAX doesn't *differentiate algebraically* like sympy, it builds and walks a graph like this:\n",
    "\n",
    "\n",
    "```\n",
    "          x = 2.0\n",
    "           |\n",
    "     +-----+------+\n",
    "     |            |\n",
    "   x**2         3*x\n",
    "     |            |\n",
    "     +-----+------+\n",
    "           |\n",
    "          +2\n",
    "           |\n",
    "         Output\n",
    "```\n",
    "Backward pass:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{df}{dx} = \\frac{∂ \\text{output}}{∂x²} * \\frac{∂x²}{∂x} + \\frac{∂ \\text{output}}{∂(3x)} * \\frac{∂(3x)}{∂x} $$\n",
    "$$\n",
    "       = 1 * 2x + 1 * 3 = 2x + 3$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb05bbd9",
   "metadata": {},
   "source": [
    "You can actually see the intermediate representation (or the JAX's computational graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ecf941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22mb\u001b[35m:f32[]\u001b[39m = integer_pow[y=2] a\n",
      "    c\u001b[35m:f32[]\u001b[39m = mul 3.0:f32[] a\n",
      "    d\u001b[35m:f32[]\u001b[39m = add b c\n",
      "    e\u001b[35m:f32[]\u001b[39m = add d 2.0:f32[]\n",
      "  \u001b[34;1min \u001b[39;22m(e,) }\n"
     ]
    }
   ],
   "source": [
    "from jax import make_jaxpr\n",
    "print(make_jaxpr(f)(2.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b085264",
   "metadata": {},
   "source": [
    "Let's see more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9af8ef8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661194 0.10499357]\n"
     ]
    }
   ],
   "source": [
    "from jax import grad\n",
    "\n",
    "def sum_logistic(x):\n",
    "    return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(derivative_fn(x_small))  # Should print the derivative of sum_logistic at x_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8b92c",
   "metadata": {},
   "source": [
    "Let's verify using finite differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "635f7431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24998187 0.1965761  0.10502338]\n"
     ]
    }
   ],
   "source": [
    "def first_finite_difference(f, x, h=1e-3):\n",
    "    return jnp.array([(f(x + h * v) - f(x - h * v)) / (2 * h) for v in jnp.eye(len(x))])\n",
    "\n",
    "print(first_finite_difference(sum_logistic, x_small))  # Should be close to the derivative computed by JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a24f49",
   "metadata": {},
   "source": [
    "We can also `jit` compile it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec308b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0207841\n"
     ]
    }
   ],
   "source": [
    "print(grad(jit(grad(jit(grad(jit(grad(sum_logistic)))))))(2.0))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659d759f",
   "metadata": {},
   "source": [
    "So the `grad()` and `jit()` can be composed and mixed arbitrarily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ce64d",
   "metadata": {},
   "source": [
    "Beyong scalar-valued function, the `jax.jacobian()` transformation can be used to compute the fulll Jacobian matrix of vector-valued functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b4cc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.        0.       ]\n",
      " [0.        2.7182817 0.       ]\n",
      " [0.        0.        7.389056 ]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacobian\n",
    "\n",
    "print(jacobian(jnp.exp)(x_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0202be0f",
   "metadata": {},
   "source": [
    "Jacobian is a matrix of all partial derivatives of a function with multiple inputs and outputs, \n",
    "\n",
    "If we have a function: \n",
    "$$ f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$$\n",
    "\n",
    "then the jacobian is an $m \\times n$ matrix:\n",
    "\n",
    "$$ J_{ij} = \\frac{\\partial f_i}{ \\partial x_j} $$\n",
    "\n",
    "It tells you how much each output depends on each input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc70c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): \n",
    "    return jnp.array([\n",
    "        x[0] + 2 * x[1],\n",
    "        x[0]**2 + x[1]**2,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856cb7f",
   "metadata": {},
   "source": [
    "The Jacobian matrix is: \n",
    "$$ \n",
    " J = \\begin{bmatrix}\n",
    "\\frac{\\partial f_0}{\\partial x_0} & \\frac{\\partial f_0}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_0} & \\frac{\\partial f_1}{\\partial x_1}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 & 2x_0 \\\\\n",
    "2 & 2x_1\n",
    "\\end{bmatrix} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e504b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [2. 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([1.0, 2.0])\n",
    "jacobian_f = jacobian(f)\n",
    "print(jacobian_f(x))  # Should print the Jacobian matrix of f at x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7742d3be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parallel_adventures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
